
# November 2016

## 2016/11/1

- Number of synapses in a typical brain = 10^15 (10^11 neurons x 7000 connections)
- Brain damage makes functions (audio, visual, etc.) relocate
- Some simple neural models: linear, binary threshold, rectified linear, sigmoid

## 2016/11/2

- Three types of learning task: supervised, reinforcement, unsupervised
- Reinforcement learning is difficult because 1) rewards are delayed and 2) a scalar reward lacks information
- Unsupervised learning has been largely ignored by the community

## 2016/11/3

- Week2
- Types of neural network architectures: feed-forward, recurrent (with loops), symmetrically connected networks
- Boltzmann machines: symmetrically connected networks with hidden units
- Perceptrons: Minsky and Paper (1969) "Perceptrons" showed what they can do and their limitation

## 2016/11/4

- Week3
- Informal proof of perceptron convergence: a "generously feasible" weight vector
    - a weight vector that lies in the feasible region ('correct' side of all training vectors)
    - by a margin = (length the input vector)
- After each update, the squared distance of all those generously feasible weight vectors is always decreased

## 2016/11/5

- Week3
- Limitations of perceptrons
    - XOR (discriminate if feature values are the same or not)
    - In general: different patters that have the same number of pixels on, if we allow translation with wrap-around (Minsky and Papert: "Group Invariance Theorem")

